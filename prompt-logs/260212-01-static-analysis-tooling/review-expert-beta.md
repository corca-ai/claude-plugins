### Expert Reviewer Beta: W. Edwards Deming

**Framework Context**: Systems thinking, common cause vs. special cause variation, the PDCA (Plan-Do-Check-Act) cycle, and the principle that quality must be built into the process rather than inspected after the fact — grounded in *Out of the Crisis* (1986) and *The New Economics* (1994).

#### Concerns (blocking)

- [Medium] **The plan adds inspection layers rather than building quality into the process.**
  Deming's central thesis in *Out of the Crisis* (Chapter 3) is that "cease dependence on inspection to achieve quality" — not by eliminating measurement, but by designing processes that produce quality inherently. This plan creates 5 standalone detection scripts that run *after* content is already written. None of the tools prevent defects at the point of creation. A broken link, an orphaned document, or a near-duplicate block has already been authored, committed, and merged before any of these scripts would catch it. The plan explicitly defers hook integration and CI/CD to future sessions, meaning there is no feedback loop connecting detection back to the authoring process. Without that loop, these tools become a classic mass-inspection regime: find defects after the fact, then rework.
  Reference: Plan sections "Out of scope" (hook integration, CI/CD deferred) and "Deferred Actions."

- [Medium] **The exit code convention (0=pass, 1=fail) conflates common cause and special cause variation.**
  Deming distinguished sharply between common cause variation (systemic, inherent to the process) and special cause variation (attributable to a specific assignable event). The plan's exit code taxonomy ("0 = success/clean, 1 = findings or tool error") treats all findings identically. A stale document that hasn't been touched in 91 days (common cause — the process does not include scheduled review) is signaled the same way as a genuinely broken link introduced by a specific commit (special cause). The duplicate detector flags blocks above a Jaccard threshold without distinguishing intentional reuse (templates, boilerplate) from accidental copy-paste drift. When the system cannot distinguish between these two types of variation, responders will either chase common-cause variation as if it were special (tampering, in Deming's terminology) or ignore special-cause signals buried in common-cause noise.
  Reference: Plan "Success Criteria > Qualitative" section on exit code taxonomy; `doc-churn.sh` classifying all files into fresh/current/stale/archival without distinguishing *why* a file is stale.

- [Low] **No operational definition of "quality" for this documentation system.**
  Deming insisted (*Out of the Crisis*, Chapter 9) that without an operational definition — a procedure for measurement that yields a repeatable result — the concept of quality is meaningless. The plan defines behavioral scenarios (BDD) for each tool, but these verify tool behavior, not documentation quality. What constitutes an acceptable orphan count? What Jaccard threshold separates harmful duplication from acceptable structural similarity? What staleness age is genuinely problematic vs. appropriate for reference material? Without operational definitions, each tool run produces numbers without context for action.
  Reference: Plan Step 7 "Integration verification" — verifies tools run, not that results are actionable.

#### Suggestions (non-blocking)

- **Close the PDCA loop before declaring the tooling complete.** The plan as written is the "Plan" and "Do" phases of PDCA. The "Check" phase (Step 7's integration verification) only confirms the tools work mechanically. The "Act" phase — feeding findings back to change the process — is entirely deferred. Deming would urge that even a minimal feedback mechanism be included in this session: at minimum, document what action should be taken for each category of finding. A runbook or decision tree ("broken internal link → fix before merge; orphan document → review in next session; stale file > 180d → add to archive candidate list") would transform these inspection tools into process improvement instruments. This need not require hook integration; a `TRIAGE.md` alongside the scripts would suffice.

- **Separate common-cause from special-cause signals in tool output.** For `doc-churn.sh`, consider distinguishing files that are stale because they are stable reference material (common cause — the process correctly leaves them alone) from files that are stale because they were abandoned mid-work (special cause). Git metadata alone can approximate this: a file last touched in a commit with message containing "archive" or "reference" is likely intentionally static. For `find-duplicates.py`, consider an allowlist mechanism (a `.duplicate-allow` file listing known-intentional overlaps) so that common-cause duplication does not pollute the signal. The plan's `--threshold 0.7` is a step in this direction, but thresholds alone cannot distinguish intent.

- **The per-step commit strategy is a strength — formalize it as explicit PDCA micro-cycles.** Each of the 7 steps is independently testable, which aligns well with Deming's iterative improvement philosophy. However, the plan does not specify what happens when a step fails verification. Adding a brief "if Step N verification fails, diagnose root cause before proceeding to Step N+1" clause would prevent the accumulation of unresolved issues across steps. This is the difference between sequential delivery (waterfall with checkpoints) and genuine PDCA (each cycle informs the next).

- **Adopt `additionalProperties: true` with monitoring, not as a permanent posture.** The plan's schema strategy — strict `required` fields but permissive `additionalProperties` — is presented as allowing "organic evolution." Deming would observe that unmonitored organic evolution is simply uncontrolled variation. Consider logging unknown additional properties as informational output (not failures) so that schema drift becomes visible. This transforms the schema validator from a gate (pass/fail) into a control chart (tracking variation over time), which is far more aligned with Deming's philosophy of statistical process control.

- **The `doc-churn.sh` tool's "exit 0 always" convention is the most Deming-aligned decision in the plan — extend this philosophy.** The plan correctly identifies `doc-churn.sh` as informational rather than a linter, and assigns it exit 0 unconditionally. This is precisely the right posture for a common-cause measurement tool: it provides data for management to study the system, not a pass/fail gate that triggers tampering. Consider whether `find-duplicates.py` and `doc-graph.mjs --orphans` should adopt the same philosophy. Orphan documents and near-duplicates are symptoms of system structure, not point defects. Treating them as failures (exit 1) incentivizes quick fixes (delete the orphan, merge the duplicate) rather than systemic improvement (restructure the information architecture, create templates for recurring patterns).

#### Framework-Specific Insights

**On building quality in vs. inspecting it out.** Deming's argument was never that measurement is wasteful — he was a statistician and devoted his career to measurement. His argument was that measurement at the *end* of a production line is wasteful because the defect has already been produced. The value of measurement is at *process control points* — places where the data can prevent the defect from occurring. This plan positions all 5 tools at the end of the line. The architectural decision to defer hook integration means that by the time `check-links.sh` runs, someone has already written a broken link, committed it, possibly built other work on top of it. The cost of remediation compounds. The most impactful single change would be to integrate even one tool (likely `check-links.sh --local`) into the authoring workflow immediately, even as a manual pre-commit check documented in `AGENTS.md`, rather than waiting for formal hook integration.

**On the system of profound knowledge.** Deming's System of Profound Knowledge has four components: appreciation for a system, knowledge of variation, theory of knowledge, and psychology. This plan addresses the first well (the tools map the documentation system's structure — the reference graph, the schema relationships, the temporal patterns). It partially addresses variation (thresholds, classifications). It does not address theory of knowledge (what predictions do these measurements enable? what hypotheses about documentation quality can we now test?) or psychology (how will contributors respond to exit-code-1 signals? will they game the metrics rather than improve the underlying process?). These are not implementation concerns for this session, but they should inform how the tools are introduced to the team.

**On the PDCA cycle and the commit strategy.** The per-step commit approach has a subtle alignment with PDCA that the plan does not make explicit. Each step creates a tool (Plan + Do), Step 7 verifies it (Check), and the deferred hook integration would be Act. But the gap between Check and Act is an entire future session. Deming would note that the longer the gap between Check and Act, the more the system drifts and the less relevant the Check data becomes. The 90-day staleness threshold in `doc-churn.sh` is itself an acknowledgment that documents drift over time — the same principle applies to the tooling plan itself. If hook integration is deferred too long, the standalone scripts will become stale artifacts themselves, detected perhaps by their own `doc-churn.sh`.

#### Provenance
- source: REAL_EXECUTION
- tool: claude-task
- expert: W. Edwards Deming
- framework: Systems thinking, common cause vs. special cause variation, PDCA, building quality into process
- grounding: *Out of the Crisis* (1986), *The New Economics* (1994)

<!-- AGENT_COMPLETE -->

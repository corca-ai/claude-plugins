### Expert β: Sidney Dekker

**Framework**: Drift into failure — systems fail not through sudden breakdowns but through incremental, locally rational adaptations that erode safety margins until the system drifts past a tipping point. Understanding failure requires reconstructing the local rationality of each actor at each decision point.
**Source**: *Drift into Failure: From Hunting Broken Components to Understanding Complex Systems* (Ashgate, 2011); *The Field Guide to Understanding 'Human Error'* (CRC Press, 3rd ed., 2014); roster-verified.
**Why this applies**: Session 260217-03 implemented prevention mechanisms for an incident (260216-04) that was itself a textbook case of practical drift — an agent incrementally deleting files under locally rational reasoning ("this file looks unused per my grep analysis"), each step sensible in isolation, accumulating into irreversible harm. The prevention session then exhibited its own drift dynamics: building on a predecessor's defective code, accepting known detection gaps as "residual risk," and losing a critical decision to compaction. The question is not whether the safety mechanisms are correct — the code review confirmed they are — but whether the *process that produced them* is itself drifting toward conditions that will defeat those mechanisms.

---

#### Moment 1: The inherited exit-code defect as practical drift made visible

The most revealing event in this session was discovering that the previous agent (S260217-02) had written `exit 0` in the blocking function of `workflow-gate.sh` — meaning the hook appeared to work, produced the right JSON output, logged the right messages, but actually blocked nothing. The session log records this as a "build-on-top" efficiency decision (CDM-1), and the lessons file frames it as a gap-analysis problem. But through the lens of practical drift, what happened is more structurally interesting.

The previous agent had drifted from the specification incrementally. It started with the correct template structure (HOOK_GROUP, gate, stdin parse, decision output). It implemented the logic correctly at the *semantic* level — the JSON output said `"decision":"block"`. But it used `exit 0` because, in the agent's local rationality, the JSON `decision` field *was* the blocking mechanism. The agent was reasoning about the *documented protocol* (which specifies JSON decision output) rather than the *actual enforcement mechanism* (which is the exit code). This is exactly the pattern I describe in *The Field Guide*: the gap between work-as-imagined (the JSON protocol) and work-as-done (the exit code) is where practical drift lives. The previous agent was not wrong about what it was trying to do; it was wrong about how the system actually works.

What makes this a drift signal rather than a simple bug is the accumulation pattern. The `exit 0` was not an isolated error — it was consistent across all blocking paths in the script. The agent had developed a mental model ("JSON decision field controls blocking") and applied it systematically. This is the hallmark of practical drift: a locally coherent misunderstanding that produces work that *looks correct to everyone except the system itself*. The current session's agent caught it, but only because it performed a systematic gap analysis against the revised plan. Without that gap analysis — which was a deliberate methodological choice, not an automatic safeguard — the defective code would have shipped as "working" prevention.

This matters for the broader CWF system because the exit-code semantics are documented in multiple places but are not enforced by any structural mechanism. A future agent writing a new hook faces the same drift risk: it can produce correct-looking JSON output while using the wrong exit code, and nothing in the hook architecture will catch it. The deletion safety hook has the same vulnerability — if a future maintainer changes `json_block` to not call `exit 1`, the hook becomes a silent observer rather than a guard. The defense against this drift is currently a convention ("exit 1 = block"), not a structure.

#### Moment 2: Accepting grep -rl limitations as "residual risk" — the normalization of deviance

The plan's header comment for `check-deletion-safety.sh` documents that `grep -rl` cannot detect variable-interpolated references like `$SCRIPT_DIR/csv-to-toon.sh`. The CDM analysis frames this as an "accepted residual risk." The retro notes it but does not flag it. The code review (James Reason) maps it as a "residual hole" in the prevention layer. Everyone agrees it is a known limitation.

From a drift-into-failure perspective, this is the beginning of what Diane Vaughan (whose work on normalization of deviance I frequently cite alongside my own) calls the normalization of deviance. The original incident — the deletion of `csv-to-toon.sh` — happened precisely because of a dynamic reference (`$SCRIPT_DIR/csv-to-toon.sh`) that escaped the agent's grep-based analysis. The prevention mechanism now being built to stop this exact failure mode has the *exact same detection gap* that enabled the failure. And everyone involved — the planner, the implementer, the six code reviewers — has noted this gap, accepted it, and moved on.

This is not wrong per se. Static analysis genuinely cannot resolve all dynamic references without execution. But the drift risk is in the framing: "accepted residual risk" is a static assessment. It assumes the gap will remain at its current size. In practice, as the codebase grows, as more scripts use variable-based sourcing patterns (which is idiomatic bash), and as agents learn to trust the hook ("the deletion safety hook didn't fire, so this file must be safe to delete"), the gap between the hook's coverage and the system's actual dependency graph will widen. The hook creates a *false floor of safety* — agents will rationally reduce their own caution because they believe the hook is protecting them, but the hook's protection has a hole exactly where the most dangerous deletions occur (dynamically-referenced files).

This is the pattern I describe in *Drift into Failure* Chapter 5: safety measures that are locally adequate when introduced become the seeds of the next failure because they change the behavior of the actors who rely on them. The deletion safety hook will reduce agent caution around deletions. That reduced caution will eventually encounter the grep limitation. The resulting incident will look like "the hook didn't catch it" — but the deeper cause will be the drift in agent behavior that the hook itself enabled.

#### Moment 3: Decision loss to compaction — the erosion of organizational memory

CDM-4 documents that the user's decision to proceed with 4/6 reviewers was lost to context compaction, causing three redundant retry attempts. The retro frames this as a "compaction recovery" problem requiring a decision journal. But through the drift lens, this is a symptom of a more fundamental pattern: the erosion of organizational memory under operational pressure.

In my framework, organizations drift into failure partly because they lose memory of *why* decisions were made. The compaction mechanism is a direct analog: it preserves what was done (actions, outputs) but discards why it was done (the reasoning, the trade-offs, the user's explicit choice). The agent after compaction was not being careless — it was being locally rational. It saw 4 completed reviews and 2 missing reviews, and it applied the standard protocol (retry missing reviews). The standard protocol was correct in general; it was wrong in this specific case because the user had already made an exception. But the exception — the local adaptation to circumstances — was exactly what compaction discarded.

This is structurally identical to the pattern I describe in organizational accidents: shift handoffs that preserve procedures but lose the situational adaptations that the previous shift made. The new shift re-applies the standard procedure, overriding the predecessor's locally rational adaptation, and the result is wasted effort or worse. The CWF pipeline has the same vulnerability at every compaction boundary: any locally rational deviation from the standard pipeline that the user approved will be lost, and the post-compaction agent will re-apply the standard pipeline.

The proposed fix (a decision journal in compaction-immune storage) addresses the symptom correctly. But the deeper drift risk is that the decision journal itself will accumulate entries that become stale, contradictory, or ambiguous — creating a new failure mode where agents follow outdated decisions because the journal says to. The organizational memory problem is not solved by adding a new storage layer; it requires a maintenance discipline for that layer (expiration, reconciliation, conflict resolution).

---

**What worked, through the drift lens**:

The session's most structurally significant achievement was the *fail-closed/fail-open asymmetry* (CDM-2). This is a counter-drift mechanism. Practical drift is driven by the local rationality of choosing the easier path — and fail-closed for deletion safety makes the easier path the *safer* path. An agent encountering a deletion block will choose to not delete (easier than investigating the block), which is exactly the safe behavior. This is the opposite of the usual drift dynamic where the easier path is the less safe one. The asymmetry with the workflow gate (fail-open) prevents the counter-drift mechanism from creating its own operational problems (bricking the agent session on every prompt). This is sophisticated safety design — it uses the drift dynamic itself as a safety mechanism rather than fighting against it.

The "fix while context is hot" pattern (CDM-3) is also a counter-drift mechanism, though a risky one. It preserves the causal chain between an operational failure and its fix, preventing the drift pattern where operational pain is noted, deferred, forgotten, and then repeated. The risk is that it creates scope creep pressure — but the session managed this risk well by keeping the fix small and explicitly scoped.

---

**Recommendations**:

1. **Create a structural exit-code enforcement test for all hooks.** The `exit 0` bug in the previous agent's code was caught by manual gap analysis, not by any automated check. Write a simple integration test that, for each hook registered in `hooks.json`, verifies that the hook's blocking path actually exits non-zero. This converts the "exit 1 = block" convention from a document-level rule (subject to practical drift) into a structural constraint (immune to drift). The test need not be complex — a shell script that invokes each hook with a synthetic blocking input and asserts the exit code is sufficient. Without this, every future hook author faces the same drift risk as S260217-02.

2. **Document the grep detection boundary as an *operational constraint* with a compensating control, not as an "accepted residual risk."** Reframe the header comment in `check-deletion-safety.sh` from "these patterns will not be detected" (passive acknowledgment) to "when deleting files that use variable-based sourcing patterns, this hook cannot protect you — manually verify callers using `grep -r` with the basename and check for `$SCRIPT_DIR`, `$DIR`, `source` patterns" (active operational guidance). Additionally, consider a second-line defense: when the hook allows a deletion (exit 0), emit an advisory message listing the detection limitations, so the agent's reduced caution is at least partially compensated by a reminder of what the hook cannot see. The goal is to prevent the normalization of deviance by keeping the detection gap visible at the point of action, not buried in a header comment that agents will not re-read after the first encounter.
<!-- AGENT_COMPLETE -->

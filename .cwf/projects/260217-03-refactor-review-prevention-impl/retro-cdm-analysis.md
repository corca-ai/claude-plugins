# CDM Analysis — 260217-03 Prevention Proposals Implementation

Session: 260217-03-refactor-review-prevention-impl
Pipeline: cwf:run (gather -> clarify -> plan -> review(plan) -> impl -> review(code) -> refactor -> retro)
Model: claude-opus-4-6 | Branch: feat/260217-03-review-prevention-impl

---

## CDM 1: Build-on-top vs reimplementation of previous agent's work

The previous agent (S260217-02) left ~500 lines of uncommitted shell scripts in the worktree — `check-deletion-safety.sh` (270 lines), `workflow-gate.sh` (171 lines), `cwf-live-state.sh` (+301 lines), along with modifications to `hooks.json`, `agent-patterns.md`, `check-links-local.sh`, `impl/SKILL.md`, `run/SKILL.md`, and `cwf-state.yaml`. The decision was whether to discard this work and reimplement from scratch, or to adopt it and fix targeted issues against the revised plan.

| Probe | Analysis |
|-------|----------|
| **Cues** | `git diff --stat HEAD` in the worktree revealed 18 modified files with substantial content. Reading the actual files showed that key scripts like `check-deletion-safety.sh` and `workflow-gate.sh` were "mostly complete" — they followed the correct template structure (HOOK_GROUP, gate, stdin parse, decision output) and covered the core logic. However, the plan review had identified 9 concerns (PostToolUse timing, rm pattern contradiction, YAML sanitization gap, etc.) that the previous agent's code did not address because it was based on the pre-revision plan. |
| **Goals** | Two competing objectives: (1) implementation velocity — the cwf:run pipeline had already consumed multiple context windows across gather, clarify, plan, and review stages, and (2) correctness assurance — the revised plan incorporated critical fixes (PostToolUse -> PreToolUse, fail-closed exit codes, sanitization integration) that the previous agent's code missed or got wrong. A third latent goal was avoiding the "sunk cost fallacy" of keeping code simply because it existed. |
| **Options** | (a) Full reimplementation from the revised plan — clean, no legacy issues, but ~500 lines of redundant work. (b) Build on top — review existing code against the revised plan, identify gaps, and apply targeted fixes. (c) Hybrid — reuse structural scaffolding but rewrite core logic sections that the review flagged as flawed. |
| **Basis** | Option (b) was chosen based on the gap analysis showing that the previous agent's code was structurally sound (correct template patterns, correct function decomposition) but had specific identifiable defects. The defects mapped directly to the 9 review concerns, making targeted fixing tractable. The session log records: "이전 에이전트가 ~500줄의 셸 스크립트를 남겨둠. 리뷰 결과를 반영하여 타겟 수정하는 것이 재작성보다 효율적이었음." However, this decision also introduced problems — the previous agent used `exit 0` in `json_block()` (which allows rather than blocks in Claude Code hooks), used `HOOK_GROUP="workflow_guard"` instead of `"deletion_safety"`, and registered the deletion hook as PostToolUse rather than PreToolUse. Each of these required discovery and correction. |
| **Knowledge** | The agent applied a systematic gap analysis methodology: `git diff --stat`, then file-by-file reading against each plan step's requirements, then targeted edits. This is a pattern from incremental code review — evaluate delta from desired state rather than total state. The lessons file records this as: "worktree에 이전 에이전트의 의미 있는 작업이 있으면 '빌드 온탑' 전략 적용. 다만 반드시 plan의 요구사항과 대조하여 차이점을 식별해야 함." |
| **Hypothesis** | If full reimplementation had been chosen, the correctness of the final output would likely have been higher on the first pass — no inherited defects from the `exit 0`/`exit 1` confusion or the PostToolUse/PreToolUse mismatch. However, it would have consumed significantly more context window budget in a session that was already experiencing multiple compactions. The build-on-top approach traded first-pass correctness for throughput, accepting that a fix commit (`65a2f2c fix(hooks): use grep -rl and fix fail-closed exit in deletion safety hook`) would be needed. |
| **Experience** | A more experienced practitioner would have front-loaded the gap analysis before touching any code — creating a checklist of every review concern mapped to specific lines in the existing files, then executing the fixes in batch. The actual execution discovered some issues reactively (the `exit 0` -> `exit 1` change was caught by a linter, not by the gap analysis). A less experienced practitioner might have adopted the code without systematic review, propagating the `exit 0` bug into the final commit. |
| **Aiding** | A "previous agent delta report" tool — one that automatically diffs existing worktree state against a plan's requirements and produces a structured gap list — would have made this decision more informed and the execution more systematic. The current process relied on manual `git diff` inspection and file-by-file reading. |

**Key lesson**: When building on a previous agent's uncommitted work, perform an explicit gap analysis against the current plan's requirements *before* starting edits. Map each review concern to specific lines in the existing code. The structural scaffolding is the reuse value; the logic details are where bugs hide.

---

## CDM 2: Fail-closed (deletion safety) vs fail-open (workflow gate) asymmetry

Proposal A's deletion safety hook (`check-deletion-safety.sh`) uses fail-closed semantics: when `grep` fails, when `jq` is missing, when wildcard patterns are detected, the hook exits 1 (blocks). Proposal E+G's workflow gate (`workflow-gate.sh`) uses fail-open for its dependency failures: when `jq` is missing or `cwf-state.yaml` is unreadable, it outputs a warning and exits 0 (allows). The code review flagged this as an inconsistency (Security, UX/DX reviewers: "Inconsistent jq-missing behavior"), but it was a deliberate design choice.

| Probe | Analysis |
|-------|----------|
| **Cues** | The plan's Decision Log did not explicitly record this asymmetry as a decision — it emerged during implementation from the spec document's BDD criteria. Proposal A's BDD: "Given grep fails or cannot parse the deletion command / Then the hook exits 1 with actionable error message (fail-closed)." Proposal E+G's BDD: "If file missing or parse fails: output `[WARNING]` and `exit 0` (advisory warning, not blocking — fail-open for parse errors since blocking would prevent all work)." The asymmetry was implicit in the spec but became visible when the code review cross-compared both hooks. |
| **Goals** | Three competing objectives: (1) safety maximization — fail-closed for everything would prevent the most errors, (2) operational continuity — fail-closed on `workflow-gate.sh` would block *every user prompt* whenever `jq` is missing or state is corrupt, effectively bricking the entire agent session, (3) proportionality — the consequence of a false negative in deletion safety (irreversible file loss) is categorically different from a false negative in workflow gate enforcement (skipping a process step, recoverable). |
| **Options** | (a) Uniform fail-closed for both hooks — maximum safety, but workflow gate fires on every prompt, so any dependency failure locks the agent. (b) Uniform fail-open for both — consistent, but deletion safety becomes advisory, defeating its purpose. (c) Asymmetric — fail-closed for irreversible actions (deletion), fail-open for process enforcement (workflow gates). (d) Fail-closed with timeout-based fallback — block initially, but allow after N seconds or after user override. |
| **Basis** | Option (c) was chosen based on consequence severity analysis. The plan comment states: "advisory warning, not blocking — fail-open for parse errors since blocking would prevent all work." The reasoning is grounded in the original incident: `csv-to-toon.sh` deletion was *irreversible* within the session (the file was gone from the worktree). A skipped workflow gate, by contrast, is detectable after the fact and recoverable — the code review stage still happens, it just happens out of sequence. The Expert Alpha (Leveson STAMP) review confirmed this framing: deletion is an irreversible unsafe control action, while gate violation is a recoverable process deviation. |
| **Analogues** | This mirrors the distinction in safety engineering between safety-critical and mission-critical systems. Aviation uses fail-closed for flight control surfaces (loss of control = catastrophic) but fail-open for in-flight entertainment (loss of movies = acceptable). The deletion hook is a flight control surface; the workflow gate is cabin lighting. |
| **Situation Assessment** | The situation was correctly assessed at design time. The code review's "inconsistency" flag was technically accurate — the behavior *is* inconsistent between hooks — but the reviewers who flagged it (3/6) focused on API uniformity rather than consequence analysis. The plan's response was to document the rationale rather than change the behavior, which is appropriate. |
| **Aiding** | A "consequence severity matrix" template for hook design — forcing explicit classification of each hook's failure mode as reversible/irreversible and its trigger frequency (per-command vs per-prompt) — would make this asymmetry intentional from the start rather than emergent during implementation. |

**Key lesson**: When designing multiple safety mechanisms, choose fail-closed vs fail-open based on consequence irreversibility and trigger frequency, not on API uniformity. Document the asymmetry and its rationale explicitly, because reviewers will flag it as an inconsistency if the reasoning is not visible.

---

## CDM 3: Adding Step 5 (adaptive CLI timeout) as mid-session scope expansion

During the plan review stage, both Codex and Gemini CLIs timed out at 120 seconds on a ~500-line prompt (plan 239 lines + spec 270 lines). The user asked "왜 타임아웃 났죠?" and, after receiving the explanation, directed: "review timeout은 문제 복잡도에 맞춰 조절하도록 해야겠네요. 이것도 이번 구현 스코프에 포함해주세요." This expanded the plan from 4 steps to 5 steps mid-pipeline.

| Probe | Analysis |
|-------|----------|
| **Cues** | The immediate trigger was the operational failure: exit code 124 from both external CLIs, with Gemini's stderr showing only "Loaded cached credentials" + "Hook registry initialized with 0 hook entries" — meaning the model never started generating. The user connected this operational pain to a systemic design flaw: the hardcoded `timeout 120` in `cwf:review`'s SKILL.md does not account for prompt size variation. The user's directive was explicit: "이것도 이번 구현 스코프에 포함해주세요 (왜 고치는지 레슨과 함께)." |
| **Goals** | Competing objectives: (1) scope discipline — the session was already implementing 4 prevention proposals from a post-incident review, and adding a 5th unrelated fix risks scope creep and reduced quality across all items, (2) operational urgency — the timeout problem was actively causing review stage failures *in this session*, making it uniquely actionable, (3) lesson capture fidelity — the user explicitly wanted the fix to be paired with its causal lesson, creating a tight feedback loop. |
| **Options** | (a) Defer to a separate session — clean scope boundaries, but loses the context of why the fix is needed. (b) Add as Step 5 in the current plan — scope expansion, but the fix is small (a scaling table + 4 string replacements in SKILL.md) and contextually motivated. (c) Apply as an ad-hoc fix without plan integration — fastest, but violates CWF pipeline discipline. |
| **Basis** | Option (b) was chosen. The plan revision added Step 5 with a clear context section: "During this session's plan review, Codex and Gemini CLIs both timed out (120s) on a ~500-line prompt. The hardcoded `timeout 120` does not account for prompt size variation across review modes." The fix itself was modest: a 3-row scaling table (< 300 lines -> 120s, 300-800 -> 180s, > 800 -> 240s) and replacing 4 hardcoded `timeout 120` references. The cost of scope expansion was low relative to the session's overall complexity (Step 3 alone modified 5 files). |
| **Knowledge** | The user applied the "fix it while the context is hot" heuristic — a known pattern in incident response where fixes discovered during investigation are implemented immediately because the operational context (why it matters, how it manifests, what the correct fix is) is maximally available. Deferring loses this context to compaction, session boundaries, and human memory. |
| **Time Pressure** | Moderate. The session had already experienced 2 context compactions and was deep into the pipeline. Adding another step increased the risk of a third compaction before completion. However, the fix's small size (a SKILL.md edit, not a new shell script) mitigated this risk. |
| **Hypothesis** | If deferred, the adaptive timeout would likely have been forgotten or de-prioritized — it is a "nice to have" fix that would be hard to justify as a standalone session. The user's directive to pair it with a lesson ("왜 고치는지 레슨과 함께") suggests awareness that the operational context would be lost. The code review synthesis later confirmed the fix worked: "BDD-11: 500-line prompt -> 180s timeout — All 6: review/SKILL.md 300-800 range" passed across all reviewers. |

**Key lesson**: When an operational failure during a session reveals a systemic design flaw, and the fix is small relative to the session's scope, add it to the current session's plan with an explicit causal lesson. The "fix it while the context is hot" pattern preserves the causal chain that deferred tickets lose.

---

## CDM 4: Proceeding with 4/6 reviewers after external CLI timeout

After Codex and Gemini both timed out during plan review, fallback agents were launched but killed during a user interrupt. The user was then asked: "Correctness/Architecture 리뷰 2개가 빠져있습니다. 어떻게 진행할까요?" with options [4개로 합성 진행, Fallback 재시도, 직접 보완]. The user chose "4개로 합성 진행" (proceed with 4 reviews). This decision was later lost to context compaction, causing the agent to re-launch fallbacks 3 times before recovering the original decision.

| Probe | Analysis |
|-------|----------|
| **Cues** | The user saw that 4 internal reviewers (Security, UX/DX, Expert Alpha/Leveson, Expert Beta/Deming) had completed successfully and provided substantive, cross-validating findings. The missing perspectives were Correctness (intended for Codex) and Architecture (intended for Gemini). The user weighed the coverage of 4 reviewers against the delay of retrying external CLIs that had already failed. |
| **Goals** | Competing objectives: (1) review coverage — the cwf:review protocol calls for 6 reviewers to ensure perspective diversity, (2) pipeline momentum — the session was multi-stage with finite context budget, and every retry consumed tokens, (3) model diversity — Codex and Gemini provide non-Claude perspectives that reduce common-mode failure in review (all 4 completed reviewers were Claude-based Task agents). |
| **Options** | (a) Proceed with 4 reviewers — fast, but reduced perspective diversity and missing the "correctness" lens entirely. (b) Retry fallbacks — more coverage, but the fallback agents are also Claude-based (Task agents), so model diversity is not recovered. (c) Retry with increased timeout — addresses the root cause, but requires modifying the review infrastructure mid-session. |
| **Basis** | The user chose option (a) pragmatically. The 4 completed reviewers had already identified the session's most critical issue (PostToolUse timing) with strong cross-reviewer consensus (3/4 flagged it independently). The synthesis verdict was "Revise" based on the 4 available reviews — meaning the plan would be revised regardless. Additional reviewers would likely have reinforced existing findings rather than uncovering novel issues. The user's decision implicitly weighed: "the marginal value of 2 more reviews is low given the existing consensus, and the pipeline needs to move forward." |
| **Situation Assessment** | The assessment was correct for the plan review but created a downstream risk. The correctness and architecture reviews were eventually re-launched in a later context window (Turn 22) and provided additional findings that reinforced the existing consensus. Critically, the *loss of this decision to compaction* was the more significant problem — it caused 3 redundant fallback retry attempts, wasting tokens and time. The user recognized this meta-problem: "컴팩트 되기 전 세션 로그 뒤쪽 읽어보세요. 왜 멈췄었는지, 어떤 의사결정했는지 컴팩트 이후에 전달이 안되는군요." |
| **Aiding** | The compaction recovery mechanism needs a "decision journal" that persists user decisions across context boundaries. The lesson was recorded: "사용자가 '4개로 합성 진행' 선택했으나 컴팩트 요약에 이 결정이 명확히 전달되지 않아, 에이전트가 fallback 재시도를 3회 반복함." A persistent decision journal in `cwf-state.yaml` (or session state) would have prevented this waste. |
| **Tools** | The cwf-live-state.sh `set` command was available for persisting decisions, but the protocol did not call for recording this specific kind of decision (user's review coverage choice). The new `user_directive` field added by Proposal E+G could serve this purpose in future sessions. |

**Key lesson**: User decisions made during a session must be persisted to compaction-immune storage (state files, not conversation context) immediately after they are made. The cost of re-asking a decided question is not just the token waste — it erodes user trust in the agent's continuity.

---

## Cross-cutting observations

1. **Compaction as a structural constraint**: Three of the four critical decisions were shaped by context compaction pressure — the build-on-top choice was partly motivated by context budget conservation, Step 5 addition was explicitly tagged with "fix while context is hot" reasoning, and the 4/6 reviewer decision was lost entirely to compaction. This suggests that compaction-awareness should be a first-class design parameter in multi-stage pipelines, not an afterthought.

2. **Review-driven plan revision as a positive feedback loop**: The plan review stage caught the PostToolUse/PreToolUse timing error, the rm pattern contradiction, and the YAML sanitization gap — all of which would have been latent defects in the final implementation. The "Revise" verdict forced plan revision before implementation, preventing defects from reaching code. The 6-reviewer, multi-perspective model (including domain experts like Leveson/STAMP and Deming) provided higher signal density than a single reviewer would have.

3. **Fail-closed/fail-open as a design vocabulary**: The explicit articulation of failure mode semantics (CDM 2) created a reusable vocabulary for future hook design. Rather than each hook independently choosing its error behavior, the consequence severity framework provides a principled basis for the choice.

<!-- AGENT_COMPLETE -->

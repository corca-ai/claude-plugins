# Section 4: CDM Analysis — S24 Static Analysis Tooling Integration

Critical Decision Method (CDM) analysis per Gary Klein's framework. 4 critical decisions identified from session S24, each analyzed with 5-8 probes citing session-specific evidence.

---

### CDM 1: Schema validation tool selection — ajv-cli + yq over yamale

At the tool selection stage, the session chose JSON Schema standard (ajv-cli for validation, yq for YAML-to-JSON conversion) over Python-native alternatives like yamale. This shaped the entire `check-schemas.sh` implementation: a multi-binary pipeline (bash wrapper calling Go binary yq and Node.js npx ajv-cli) instead of a single-language solution.

| Probe | Analysis |
|-------|----------|
| **Cues** | The prior-art research phase (`plan-prior-art-research.md`) documented the ajv-cli + yq pipeline pattern explicitly, including `yq -o json cwf-state.yaml \| npx ajv-cli validate` as the reference invocation. The `scripts/` directory already contained bash and Node.js artifacts, establishing multi-language precedent. The decision was also informed by the discovery that JSON Schema is the only schema language with cross-toolchain support (VS Code, CI validators, language servers). |
| **Knowledge** | JSON Schema is a W3C-adjacent standard with broad ecosystem support. yamale is Python-only, with no JSON Schema export path. The session participant knew that schema files would eventually be consumed by hook integrations and CI pipelines (deferred to future sessions per `plan.md` "Deferred Actions"), meaning portability of the schema format mattered more than portability of the validation runtime. |
| **Goals** | Competing objectives: (1) minimize runtime dependencies — yamale requires only Python, while ajv-cli + yq requires Node.js + Go binary; (2) maximize schema reusability — JSON Schema files can be consumed by any JSON Schema-aware tool, while yamale schemas are locked to the yamale runtime; (3) maintain convention consistency — the repo already used Node.js (`scripts/package.json` for doc-graph.mjs). Goal (2) won because the session was explicitly building infrastructure for future sessions. |
| **Options** | Three options were available: (A) yamale — single Python dep, custom schema syntax, zero portability; (B) ajv-cli + yq — two deps (Go + Node.js), JSON Schema standard, full portability; (C) Cerberus or Pydantic — Python-native with JSON Schema export, but the export is lossy for complex constraints. Option B was chosen. Option C was not explicitly considered in the session artifacts but would have been a viable middle ground. |
| **Basis** | The `plan.md` Step 4 header states "JSON Schema standard and broader tooling support" as the rationale. The architecture review (`review-architecture.md`) confirmed this was the right call: "the differentiation between `additionalProperties: true` ... versus `additionalProperties: false` ... reflects strategic reasoning" — a capability only available through JSON Schema's granular property controls, which yamale lacks. |
| **Experience** | A less experienced practitioner would have chosen yamale for its simplicity (single `pip install`, YAML-native schema syntax). A more experienced practitioner might have also considered whether the two-binary pipeline (yq + npx) creates a fragile dependency chain — which it did: the architecture review flagged `check-schemas.sh` line 128 where `yq -o json "$data_path" > "$tmpfile" 2>&1` redirected yq's stderr into the JSON temp file, producing invalid JSON on yq warnings. This exact bug was a consequence of the multi-tool pipeline complexity that yamale would have avoided. |
| **Hypothesis** | If yamale had been chosen: simpler implementation (~40 fewer lines in check-schemas.sh), no need for the `validate_target` colon-delimited encoding that Ousterhout's review flagged as a "shallow abstraction" (`review-expert-alpha.md` C2), but schemas would be non-portable and the `$comment` documentation pattern in `hooks.schema.json` (praised by both expert reviewers) would not have been possible. The session would have traded implementation simplicity for architectural dead-end. |
| **Aiding** | A decision matrix weighing "number of runtime deps" vs. "schema format portability" vs. "ecosystem maturity" would have made this trade-off explicit earlier. The prior-art research documented the tools but did not frame the selection as a trade-off analysis. |

**Key lesson**: When building validation infrastructure that will be consumed by future tooling, optimize for format portability over runtime simplicity. A standard schema format (JSON Schema, OpenAPI, etc.) outlives the specific validator chosen to enforce it.

---

### CDM 2: datasketch threshold raised from 0.5 to 0.7 after false-positive contamination

During implementation verification (Step 7), the initial MinHash/LSH threshold of 0.5 produced 713 false-positive duplicate pairs, traced to `node_modules/` README files being scanned by `pathlib.rglob()`. The threshold was raised to 0.7, and `node_modules/` exclusion was added. This was a direction change triggered by empirical evidence — a classic intent-result gap.

| Probe | Analysis |
|-------|----------|
| **Cues** | The immediate cue was the volume of output: 713 duplicate pairs from a repo with ~66 non-prompt-log markdown files. This was orders of magnitude above the expected signal. Investigation revealed that `pathlib.rglob("*.md")` followed filesystem paths regardless of `.gitignore`, pulling in `scripts/node_modules/` README files — npm packages often contain markdown with boilerplate that triggers near-duplicate detection. The `lessons.md` entry "node_modules must be excluded from all file-scanning scripts" captures this discovery. |
| **Knowledge** | The prior-art research (`plan-prior-art-research.md` section 2) documented threshold semantics: "0.5: Catches loosely similar documents ... 0.7: Standard near-duplicate threshold (recommended starting point)." The session initially chose 0.5 (more aggressive) and had to retreat to the recommended default. The knowledge was available but the initial decision overrode it. |
| **Goals** | Competing objectives: (1) high recall — catch all meaningful duplication, favoring 0.5; (2) high precision — avoid false positives that erode trust in the tool, favoring 0.7+; (3) first-run usability — the first invocation of a new tool sets user expectations, and 713 findings would have undermined credibility. Goal (3) was the proximate trigger for the correction, but goal (2) is the structural concern. |
| **Options** | Three remediation paths: (A) raise threshold only (0.5 to 0.7) — reduces false positives but does not fix the root cause; (B) add `node_modules/` exclusion only — fixes the contamination source but leaves threshold at 0.5, still producing noise from intentional template reuse; (C) both — raise threshold AND add exclusion. Option C was chosen. The `lessons.md` entry confirms both fixes were applied. |
| **Basis** | The dual fix addresses both the immediate contamination (exclusion) and the structural noise floor (threshold). The `plan.md` Step 3 was updated to reflect `--threshold 0.7` as the default. The correctness review (`review-correctness.md` S1) later noted that at 128 permutations, the expected error rate is ~8.8%, meaning pairs with true similarity between 0.61 and 0.79 can be misclassified — providing quantitative justification for the 0.7 threshold as a safe distance from the noise floor. |
| **Situation Assessment** | The initial situation assessment was incorrect: the assumption was that `.gitignore`-excluded directories would not appear in filesystem scans. This is a common misconception — `.gitignore` is a git-layer concept, not a filesystem-layer concept. `pathlib.rglob()`, `find`, and similar tools operate at the filesystem layer. The `lessons.md` entry explicitly corrects this: "Every script that scans for `.md` files must explicitly exclude `node_modules/` and `.git/`." |
| **Hypothesis** | If the threshold had stayed at 0.5 without the exclusion fix: the tool would produce hundreds of false positives on every run, users would stop trusting or running it, and the entire dedup capability would become shelfware. If only the exclusion was added (keeping 0.5): the tool would still flag intentional structural similarities (template patterns, boilerplate headers shared across SKILL.md files) as duplicates, creating a different class of noise that Deming's review (`review-expert-beta.md`) identified as "common cause variation" masquerading as findings. |

**Key lesson**: When a detection tool produces an order-of-magnitude more findings than expected, investigate the input set before adjusting the sensitivity. Data contamination (scanning unintended files) and threshold miscalibration are independent failure modes that require independent fixes.

---

### CDM 3: Synthesizing review conclusions from 2/6 valid review files

4 of 6 review agents (Security, UX/DX, Correctness fallback, Expert Beta/Parnas) exhausted their 12-turn budgets without writing output files, or wrote stale plan-review content instead of code-review content. The session had to synthesize a review verdict from only 2 complete code reviews (Architecture, Expert Alpha/Ousterhout) plus partial signals from agent activity logs. This is the session's most significant intent-result gap.

| Probe | Analysis |
|-------|----------|
| **Cues** | The cue was the absence of expected output files. After the code-review phase completed, only `review-architecture.md` and `review-expert-alpha.md` contained code-level analysis. `review-security.md`, `review-ux-dx.md`, and `review-correctness.md` contained plan-phase review content (written during the earlier plan-review round) that was never overwritten. `review-expert-beta.md` contained a Deming-framework analysis that was written during plan review and persisted unchanged. The session summary confirms: "4 of 6 review agents exhausted their 12-turn budget without writing output files." |
| **Knowledge** | The CWF review pipeline uses identical filenames for plan-review and code-review phases (`review-security.md`, etc.). The participant knew that the 12-turn budget was originally calibrated for plan review (shorter diffs, simpler analysis) and had not been adjusted for code review (1,515-line diffs across 15 new files). The waste analysis (Section 3, Waste Item 1) traces this to a structural root cause: "max_turns is fixed at 12 for all review modes." |
| **Goals** | Competing objectives: (1) thoroughness — wait for or re-run all 6 reviews to get complete coverage; (2) forward progress — synthesize from available data and proceed; (3) signal integrity — ensure the synthesized verdict is not biased by which reviews happened to complete. The session chose (2), accepting reduced coverage to maintain session momentum. |
| **Options** | Four options: (A) re-run failed agents with higher turn budgets — would have required CWF infrastructure changes mid-session; (B) manually perform the missing reviews — human effort substituting for agent failure; (C) synthesize from available data — accept partial coverage; (D) declare the review phase failed and skip to retro — abandon review quality signal. Option C was chosen. The session summary notes: "With only 2/6 review files valid, synthesized from 2 complete reviews + partial findings from agent activity logs." |
| **Basis** | The 2 completed reviews (Architecture + Ousterhout) happened to be the most architecturally substantive — they identified the `doc-graph.mjs` shallow module pattern, the `check-schemas.sh` target encoding leak, the `check-links.sh --json` stdout contamination, and the `yq 2>&1` redirect bug. These covered the most impactful concerns. The Security and UX/DX plan-review content, while stale, still contained valid observations (dependency pinning, naming conventions) that applied to the implementation. Synthesis was feasible because the completed reviews were deep, not because the failure was harmless. |
| **Time Pressure** | Yes. The session was already deep into the CWF pipeline (gather, clarify, plan, review-plan, impl, review-code). Re-running failed agents would have extended the session significantly. The Expert Beta (Parnas) agent had spent all 12 turns on web research (fetching ACM papers, encountering 403/404 errors) without ever reaching code analysis — a sunk-cost scenario where retrying without fixing the root cause would likely produce the same outcome. |
| **Aiding** | Two structural fixes would have prevented this: (1) mode-dependent `max_turns` — code review with large diffs needs 20+ turns, not 12; (2) namespace review output files by mode (`review-security-plan.md` vs. `review-security-code.md`) to prevent stale content from masquerading as current. Both are documented in the waste reduction section as structural fixes. A pre-review "diff size check" that adjusts turn budgets dynamically would be the most robust solution. |

**Key lesson**: When a multi-agent review pipeline produces partial results, the synthesis quality depends on which agents completed, not how many. Two deep, architecturally-focused reviews can be more valuable than six shallow ones. However, the structural fix is to ensure all agents can complete — partial success should be treated as a system failure to diagnose, not a normal operating mode.

---

### CDM 4: additionalProperties: true everywhere except hooks.schema.json

The schema design used `additionalProperties: true` (permissive) for `cwf-state.schema.json` and `plugin.schema.json`, but `additionalProperties: false` (strict) for the `hooks` object within `hooks.schema.json`. This asymmetric design was a deliberate trade-off between detecting configuration drift and allowing organic field addition.

| Probe | Analysis |
|-------|----------|
| **Cues** | The cue was the nature of the three validated files: `cwf-state.yaml` is a living document that accumulates new fields as the project evolves (24+ sessions have added fields organically); `plugin.json` follows a loose plugin manifest convention with no authoritative spec; `hooks.json` maps to a fixed runtime API (Claude hooks runtime) with exactly 7 defined event types. The constraint profile differs fundamentally: cwf-state.yaml and plugin.json are author-controlled, while hooks.json is runtime-controlled. |
| **Knowledge** | JSON Schema's `additionalProperties` is a binary gate — either unknown fields are allowed or they cause validation failure. There is no built-in "warn on unknown but don't fail" mode. The participant knew that `additionalProperties: false` at the top level of cwf-state.yaml would break validation every time a new field was added organically during a session, creating friction that would lead to either abandoning the schema or constantly updating it. |
| **Analogues** | This mirrors the classic strictness debate in API versioning: "be liberal in what you accept, conservative in what you send" (Postel's law). The hooks runtime is a "sender" with a fixed vocabulary — being conservative (strict) is correct. cwf-state.yaml is a "receiver" of organic additions — being liberal (permissive) is correct. The Deming review (`review-expert-beta.md`) drew a different analogy: "unmonitored organic evolution is simply uncontrolled variation" — arguing that permissive validation without observability is not liberality but blindness. |
| **Goals** | Three competing goals: (1) catch structural drift early — strict schemas detect unknown fields, which may indicate typos or misunderstandings; (2) allow organic evolution — new fields should not require a schema update before the code that uses them; (3) maintain developer trust — false-positive validation failures erode tool credibility (the same dynamic as the datasketch threshold in CDM 2). The session resolved this by applying strictness only where the vocabulary is genuinely closed (hooks event types). |
| **Options** | Four approaches: (A) `additionalProperties: false` everywhere — maximum strictness, high maintenance burden; (B) `additionalProperties: true` everywhere — maximum permissiveness, minimal drift detection; (C) asymmetric (chosen) — strict where vocabulary is closed, permissive elsewhere; (D) `additionalProperties: true` everywhere with logging of unknown fields (suggested by Deming review as "control chart" approach). Option D was not implemented but represents a superior long-term solution. |
| **Basis** | The `hooks.schema.json` includes a `$comment` field: "Unknown event names indicate drift -- the Claude hooks runtime only supports these 7 event types." This was praised by both the Ousterhout review ("one of the strongest design decisions in the implementation") and the architecture review ("prior plan-phase concern [C2] was addressed"). The explicitness of the rationale — documenting *why* this boundary is strict — is what elevated this from a configuration choice to a design decision. |
| **Experience** | A less experienced practitioner would likely have chosen option A (strict everywhere) or option B (permissive everywhere), not recognizing that the appropriate strictness level depends on whether the validated vocabulary is open or closed. A more experienced practitioner might have implemented option D — treating unknown fields as informational signals rather than binary pass/fail — which is what Deming's review explicitly recommended: "logging unknown additional properties as informational output (not failures) so that schema drift becomes visible." |

**Key lesson**: Schema strictness should match vocabulary closure. Use `additionalProperties: false` only when the set of valid keys is definitively enumerable and runtime-controlled. For organically evolving configurations, combine `required` field enforcement (catching missing fields) with `additionalProperties: true` (tolerating new fields), and add observability for unknown fields as a separate concern from validation.

---

## Summary of Reusable Heuristics

| # | Heuristic | Source Decision |
|---|-----------|-----------------|
| 1 | When building validation infrastructure, optimize for format portability over runtime simplicity. | CDM 1: ajv-cli + yq over yamale |
| 2 | When a detection tool produces an order-of-magnitude more findings than expected, investigate the input set before adjusting sensitivity. | CDM 2: Threshold 0.5 to 0.7 |
| 3 | Partial multi-agent results should be treated as a system failure to diagnose, not a normal operating mode. | CDM 3: 2/6 review synthesis |
| 4 | Schema strictness should match vocabulary closure — strict for closed sets, permissive-with-observability for open sets. | CDM 4: additionalProperties asymmetry |

<!-- AGENT_COMPLETE -->

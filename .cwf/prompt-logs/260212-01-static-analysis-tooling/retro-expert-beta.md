### Expert beta: David Parnas

**Framework**: Information hiding and modular decomposition criteria
**Source**: "On the Criteria To Be Used in Decomposing Systems into Modules" (CACM, 1972)
**Why this applies**: S24 decomposed static analysis into 5 independent scripts -- a decomposition decision that can be evaluated against Parnas's criteria.

The 1972 paper presents two decomposition strategies for the same system. Decomposition 1 follows the processing steps: each module corresponds to a sequential phase of the computation. Decomposition 2 follows design decisions: each module encapsulates a "secret" -- a design decision likely to change -- and exposes only a stable interface. The paper demonstrates that Decomposition 2 produces modules that are independently developable, independently changeable, and comprehensible in isolation, while Decomposition 1 produces modules that require knowledge of shared data representations and processing order.

Session S24's decomposition of static analysis into five scripts -- check-links.sh, doc-graph.mjs, find-duplicates.py, check-schemas.sh, doc-churn.sh -- is a clear instance of Decomposition 2. Each script's "secret" is its detection algorithm and the external tool it wraps: lychee's link validation engine and .lychee.toml configuration format; remark's AST traversal and unified pipeline for reference extraction; datasketch's MinHash/LSH algorithm for similarity computation; ajv-cli's JSON Schema validation with yq's YAML-to-JSON conversion bridge; and git log's commit history query format with epoch-based staleness classification. None of these secrets leak through the shared interface. A caller invokes any script with --help, --json, or no flags and receives either structured JSON or human-readable output with exit code 0 (clean) or 1 (findings). The caller does not need to know that check-links.sh delegates to a Rust binary, that find-duplicates.py uses 128-permutation MinHash signatures with word-level k-shingles, or that check-schemas.sh pipes YAML through a Go binary before validating it with a Node.js tool. This is precisely the property my 1972 criterion demands: the interface should not change when the hidden design decision changes. If lychee were replaced with another link checker, only check-links.sh would change. If MinHash were replaced with SimHash or tf-idf cosine similarity, only find-duplicates.py would change. The module boundary -- the CLI interface -- remains stable.

However, the session also produced two instructive failures of information hiding that my framework predicts will cause maintenance cost. First, the check-schemas.sh `validate_target` function accepts a colon-delimited string `"schema_file:data_file[:converter]"` (lines 84-91 and 97-107 of the implementation). This encoding forces the caller -- the loop at line 154 -- to know the internal structure of a validation target: which file is the schema, which is the data, which converter to apply, and how these fields are delimited. The "secret" that should be hidden is the conversion strategy: whether a file needs YAML-to-JSON transformation before validation. Instead, this decision is encoded in the target specification and re-parsed inside validate_target via bash string expansion (`${spec%%:*}`, `${rest#*:}`), with a detection hack where `converter == data` signals the absence of a converter (line 105). In my 1972 terminology, this is a shared data representation between the module's interface and its implementation -- exactly the coupling pattern that Decomposition 1 exhibits. The deeper module design would accept (schema_path, data_path) as positional arguments and internally inspect the data file's extension to decide whether YAML-to-JSON conversion is needed. The converter is an implementation detail, not an interface parameter. Second, the review pipeline filename collision -- where plan-review and code-review phases both write to `review-security.md`, `review-ux-dx.md`, etc. -- is a namespace sharing violation. Each review phase is a module with its own lifecycle, but they share an output namespace without encapsulation. When code-review agents failed (4 of 6 exhausted their turn budgets), stale plan-review artifacts persisted in the shared namespace, undetectable by the pipeline. My framework would require each phase to hide its artifacts behind its own interface -- either by namespacing filenames (`review-security-plan.md` vs `review-security-code.md`) or by placing them in phase-specific directories. The shared namespace is a secret that both modules must know about, creating the exact inter-module dependency my criterion forbids.

A subtler finding concerns what I would call a "correctly applied hiding decision" versus an "incompletely applied hiding decision." The `additionalProperties: true` in cwf-state.schema.json and plugin.schema.json deliberately hides from schema consumers which fields exist beyond the required set -- consumers need not know and must not depend on the optional fields. The `additionalProperties: false` in hooks.schema.json deliberately reveals the complete vocabulary: the Claude hooks runtime defines exactly 7 event types, and this closed set is part of the interface contract, not an implementation detail. This asymmetry is correct information hiding: hide what is likely to change (organic fields in cwf-state.yaml), reveal what is stable and definitional (the hooks runtime API). The `$comment` documenting the rationale ("Unknown event names indicate drift") makes the hiding decision explicit, which is itself a form of interface documentation. Where the hiding is incomplete, however, is the replicated CLI convention across all five scripts. The TTY-guarded color pattern, the --json flag semantics, the exit code taxonomy, and the usage() extraction via sed -- these are design decisions encoded identically in five places across three languages. Each replication is a point where the "secret" of how output formatting works has leaked from a single module into five. If the convention changes (for example, to support the NO_COLOR environment variable standard), all five scripts must be updated independently. In a Decomposition 2 approach, this shared convention would itself be a module -- a formatting interface whose secret is the terminal capability detection logic. The pragmatic difficulty of sharing a module across bash, Node.js, and Python does not change the analytical observation: this is a design decision that has leaked across module boundaries, and it will cost proportionally each time it needs to change.

**Recommendations**:
1. Eliminate the colon-delimited target encoding in check-schemas.sh. Replace `validate_target "schema:data:converter"` with a function that accepts positional arguments -- `validate_target "$schema" "$data" "$converter"` -- or, more deeply, three named functions (`validate_yaml`, `validate_json`) that each hide their conversion strategy entirely. The current encoding makes the caller responsible for knowledge that belongs inside the module.
2. Namespace review artifacts by pipeline phase. Use `review-security-plan.md` / `review-security-code.md` or place each phase's outputs in a subdirectory (`plan-reviews/`, `code-reviews/`). The shared filename is a shared data representation between two independent modules -- the plan-review phase and the code-review phase -- and it creates the undetectable staleness failure observed in this session.
3. If the script suite grows beyond its current five, extract the shared output convention (TTY detection, --json semantics, color initialization, exit code taxonomy) into a single authoritative source per language -- a shell library sourced by bash scripts, a shared module imported by Node.js scripts, a base class or utility module for Python scripts. At the current scale this is acceptable technical debt; at ten scripts it becomes a maintenance multiplier.

<!-- AGENT_COMPLETE -->

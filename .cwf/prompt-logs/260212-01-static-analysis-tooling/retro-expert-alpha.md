### Expert alpha: W. Edwards Deming

**Framework**: Systems thinking -- common cause vs special cause variation, PDCA, build quality into process
**Source**: *Out of the Crisis* (MIT Press, 1986), particularly Point 3 (cease dependence on inspection) and Chapter 11 (common causes and special causes)
**Why this applies**: S24 introduced 5 inspection tools into a documentation workflow -- a direct test case for Deming's tension between inspection and process quality.

The most revealing aspect of S24 is not the 5 tools themselves but the structural pattern they expose: a system that has accumulated 494 markdown files and 9,567 cross-references without any automated quality signal, then responds by bolting on 5 post-hoc detection scripts. This is the exact pattern Deming warned against in Point 3 of his Fourteen Points -- "cease dependence on inspection to achieve quality." The plan explicitly defers hook integration, CI/CD, and the `cwf:validate` orchestrator to future sessions, meaning these tools can only be run manually, after content is authored and committed. A broken link has already been written, committed, and possibly built upon before `check-links.sh` can detect it. An orphaned document has already diverged from the reference graph before `doc-graph.mjs` can flag it. This is mass inspection at the end of the production line, not quality built into the authoring process. The Deming-aligned alternative would be to integrate even one tool -- `check-links.sh --local` is the obvious candidate, given its 92-line depth and sub-second execution -- into the authoring workflow immediately, as a PreToolUse or PostToolUse hook, so that defects are caught at the point of creation rather than discovered in a separate analysis pass. The plan's own Expert Beta review (which was, ironically, the Deming-framework review during the plan phase) made exactly this recommendation. It went unimplemented.

The 4/6 review agent failure is the session's clearest demonstration of common cause versus special cause variation. All 6 agents operated under the same system constraint: a 12-turn budget originally calibrated for plan review (shorter diffs, ~200-line plans), applied unchanged to code review (1,515-line diff across 15 new files). Four agents failed identically -- they exhausted their budgets performing prerequisite work (reading files, running diffs) and never reached the output-writing phase. This is textbook common cause variation: the failures are attributable to the system design (fixed turn budget regardless of task complexity), not to any individual agent's performance. Management action on common cause variation must change the system -- in this case, implementing mode-dependent turn budgets (`max_turns` scaled to diff size) or a two-phase execution model (exploration budget + writing budget). The CDM analysis correctly identifies this: "partial success should be treated as a system failure to diagnose, not a normal operating mode." Contrast this with the Expert Beta/Parnas agent, which failed for a genuinely special cause: it spent all 12 turns attempting web research against academic sites that returned 403 errors. That failure is attributable to an assignable event (403 from ACM/IEEE sites) and requires a different intervention (URL allowlists, or routing academic research to tools with institutional access). Treating the 4 common-cause failures and the 1 special-cause failure as the same kind of problem -- "agents failed" -- is precisely the tampering Deming warned about: adjusting for common cause variation as if it were special cause, or vice versa. The system fix (scale turn budgets) and the special-cause fix (academic site access) are independent interventions.

The datasketch threshold adjustment from 0.5 to 0.7 is a complete PDCA cycle executed within a single session, and it illustrates both the power and the limits of inspection-based quality. Plan: set threshold at 0.5 to maximize recall. Do: run the tool. Check: 713 false-positive pairs, traced to `node_modules/` contamination via `pathlib.rglob()` scanning `.gitignore`-excluded directories. Act: dual fix -- add `node_modules/` exclusion (addressing the contamination source) and raise threshold to 0.7 (addressing the structural noise floor). This is the PDCA cycle working as intended. But notice the lesson it teaches: the 0.5 threshold was set during planning without empirical data, and the correction required running the tool against real data. The plan-phase prior-art research (`plan-prior-art-research.md`) had actually documented 0.7 as the "standard near-duplicate threshold (recommended starting point)" -- the knowledge was available but was overridden in favor of higher recall. The PDCA cycle corrected this, but only because someone was paying attention to the output magnitude (713 pairs from 66 files is obviously wrong). If the false-positive count had been, say, 40 instead of 713, it might have been accepted as signal when it was actually noise at a subtler level. This is why Deming argued that process control is superior to inspection: a control chart would have flagged the 0.5 threshold as out of specification before the tool ever ran on real data, by comparing the expected pairs-per-document ratio against the observed ratio. The session did PDCA correctly, but it did so reactively rather than proactively.

The `additionalProperties` asymmetry in schema design is the session's best example of statistical process control thinking applied to configuration. The hooks runtime defines a closed vocabulary of exactly 7 event types -- `additionalProperties: false` monitors this boundary because any variation is definitionally a defect (a typo or an unsupported event name). The `cwf-state.yaml` and `plugin.json` files have open vocabularies that grow organically across sessions -- `additionalProperties: true` permits this natural variation while `required` fields catch structural drift (missing essential fields). This is the Deming distinction between a process that is in control (hooks: stable, predictable, bounded) and a process that is evolving (cwf-state: growing, adaptive, unbounded). Applying the same control limits to both would be tampering: treating organic evolution in cwf-state as if it were defective variation. The `$comment` in `hooks.schema.json` -- "Unknown event names indicate drift -- the Claude hooks runtime only supports these 7 event types" -- is particularly Deming-aligned because it documents the *rationale* for the control limit, not just the limit itself. However, the Deming-complete solution would go further: log unknown additional properties in `cwf-state.yaml` as informational observations (not failures), creating a control chart of schema evolution over time. The current binary approach (pass or fail) discards information about how the configuration is evolving, which is precisely the data management needs to understand the system.

**Recommendations**:
1. **Close the inspection-to-prevention gap immediately.** Integrate `check-links.sh --local` as a PostToolUse hook for Write/Edit operations on `.md` files before the next session. This single integration converts one inspection tool into a process-quality mechanism -- defects caught at authoring time, not discovered in post-hoc analysis. The tool's 92-line depth and sub-second local execution make it the lowest-friction candidate. Do not wait for the deferred "hook integration session" -- every session between now and then accumulates broken links that will cost more to fix later. This is Deming's Point 3 in practice: cease dependence on running `check-links.sh` manually after the fact; build the link check into the authoring process.
2. **Implement mode-dependent turn budgets for the review pipeline.** The 4/6 agent failure is a common cause -- the system produces it, not the agents. Measure the diff size before dispatching review agents and scale `max_turns` proportionally (e.g., 12 turns for diffs under 500 lines, 24 turns for diffs under 2000 lines, 36 turns above). This is a system-level intervention on common cause variation. Separately, address the Expert Beta special cause by maintaining a URL allowlist or routing academic research tasks to agents with appropriate access. Do not apply the same fix to both failure modes.
3. **Add observability for `additionalProperties: true` schemas.** Modify `check-schemas.sh` to log unknown (non-required, non-documented) properties as informational output when validating `cwf-state.yaml` and `plugin.json`. This transforms the schema validator from a pass/fail gate into a control chart that tracks configuration evolution over time. The current binary approach discards the very data that would let you distinguish healthy organic evolution from uncontrolled drift -- exactly the data Deming's system of profound knowledge requires for process improvement.

---

#### Provenance
- source: REAL_EXECUTION
- tool: claude-code
- expert: W. Edwards Deming
- framework: Systems thinking -- common cause vs special cause variation, PDCA cycle, build quality into process
- grounding: *Out of the Crisis* (MIT Press, 1986) -- Point 3 (Cease Dependence on Inspection), Chapter 11 (Common Causes and Special Causes of Improvement), Chapter 3 (the PDCA cycle), and the System of Profound Knowledge framework

<!-- AGENT_COMPLETE -->
